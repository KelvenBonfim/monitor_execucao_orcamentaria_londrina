#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# v1.1 ‚Äî sniff de separador, utf-8-sig, normaliza√ß√£o de colunas, ‚Äútotal‚Äù opcional, cria/atualiza staging.

import os, sys, re, json, traceback, unicodedata
from pathlib import Path
from datetime import datetime
import argparse
import pandas as pd
import numpy as np
import psycopg2, psycopg2.extras as pgx

# ---------- CLI ----------
def parse_args():
    p = argparse.ArgumentParser(description="Carrega CSVs do Equiplano/Anexo10 para tabelas de staging.")
    p.add_argument("--csv", required=True, help="Pasta raiz com receitas/, empenhadas/, liquidadas/, pagas/")
    p.add_argument("--schema", default="public")
    p.add_argument("--verbose", action="store_true")
    p.add_argument("--dedupe", action="store_true", help="Remove linhas com a palavra 'total'")
    p.add_argument("--add-year", action="store_true", help="Cria/normaliza coluna 'ano' a partir de 'exercicio' ou do nome do arquivo")
    p.add_argument("--numeric", action="store_true",
                   help="Limpa colunas monet√°rias (remove milhares e troca v√≠rgula por ponto) ‚Äî mant√©m como texto na staging")
    return p.parse_args()

# ---------- Conex√£o ----------
def get_conn():
    url = os.getenv("DATABASE_URL")
    if not url:
        print("‚ùå Defina DATABASE_URL", file=sys.stderr); sys.exit(1)
    if url.startswith("postgresql+psycopg2://"):
        url = url.replace("postgresql+psycopg2://","postgresql://",1)
    return psycopg2.connect(url)

# ---------- Utils ----------
TOTAL_PAT = re.compile(r"(?i)\btotal\b")
DATE_IN_NAME = re.compile(r"(\d{4}-\d{2}-\d{2})")
YEAR_IN_PATH = re.compile(r"(\d{4})")
MONEY_LIKE = re.compile(r'^\s?-?\d{1,3}(\.\d{3})*,\d{2}\s?$')  # 1.234.567,89
LOG_DIR = Path("logs"); LOG_DIR.mkdir(exist_ok=True)

def strip_total_rows(df):
    if df.empty: return df
    mask = pd.Series(False, index=df.index)
    for c in df.columns:
        if pd.api.types.is_string_dtype(df[c]):
            mask |= df[c].fillna("").str.contains(TOTAL_PAT)
    return df.loc[~mask].copy()

def infer_year(path: Path):
    m = DATE_IN_NAME.search(path.name)
    if m:
        try: return datetime.strptime(m.group(1), "%Y-%m-%d").year
        except: pass
    m = YEAR_IN_PATH.search(path.as_posix())
    if m:
        y = int(m.group(1))
        if 1900 <= y <= 2100: return y
    return None

def slugify(s: str) -> str:
    # s√≥ para compara√ß√£o, n√£o uso para renomear (mantenho acentos nos nomes)
    return ''.join(ch for ch in unicodedata.normalize('NFKD', s) if not unicodedata.combining(ch))

def log_error(rel: Path, err: Exception, ctx: dict):
    p = LOG_DIR / ("__".join(rel.parts) + ".log")
    p.parent.mkdir(parents=True, exist_ok=True)
    with open(p, "w", encoding="utf-8") as fh:
        fh.write(json.dumps({
            "file": rel.as_posix(),
            "error": str(err),
            "traceback": traceback.format_exc(),
            "context": ctx
        }, ensure_ascii=False, indent=2))

# ---------- DB helpers ----------
def existing_cols(conn, schema, table):
    with conn.cursor() as cur:
        cur.execute("""
          SELECT column_name FROM information_schema.columns
           WHERE table_schema=%s AND table_name=%s ORDER BY ordinal_position
        """, (schema, table))
        return [r[0] for r in cur.fetchall()]

def ensure_table_and_cols(conn, schema, table, cols, verbose=False):
    cols = [str(c) for c in cols]
    cur_cols = existing_cols(conn, schema, table)
    if not cur_cols:
        with conn.cursor() as cur:
            cur.execute(f'CREATE SCHEMA IF NOT EXISTS "{schema}";')
            coldef = ", ".join(f'"{c}" TEXT' for c in cols)
            cur.execute(f'CREATE TABLE IF NOT EXISTS "{schema}"."{table}" ({coldef});')
        conn.commit()
        if verbose: print(f"üóÉÔ∏è criada {schema}.{table} com {len(cols)} colunas")
        return
    # adiciona colunas novas se aparecerem
    missing = [c for c in cols if c not in cur_cols]
    if missing:
        with conn.cursor() as cur:
            for c in missing:
                cur.execute(f'ALTER TABLE "{schema}"."{table}" ADD COLUMN IF NOT EXISTS "{c}" TEXT;')
        conn.commit()
        if verbose: print(f"üß± {schema}.{table}: +{len(missing)} colunas: {missing}")

def insert_df(conn, schema, table, df):
    if df.empty: return 0
    cols = list(df.columns)
    records = []
    for row in df.itertuples(index=False, name=None):
        rec = []
        for v in row:
            if pd.isna(v) or v == "":
                rec.append(None)
            else:
                rec.append(str(v))
        records.append(tuple(rec))
    with conn.cursor() as cur:
        pgx.execute_values(
            cur,
            f'INSERT INTO "{schema}"."{table}" ({",".join(f"""\"{c}\"""" for c in cols)}) VALUES %s',
            records, page_size=1000
        )
    conn.commit()
    return len(records)

# ---------- sniff de separador/encoding ----------
def sniff_sep_and_encoding(path: Path):
    # tenta utf-8-sig; se falhar, latin-1
    encodings = ["utf-8-sig", "latin-1"]
    with open(path, "rb") as fh:
        raw = fh.read(4096)
    sample = None
    for enc in encodings:
        try:
            sample = raw.decode(enc)
            encoding = enc
            break
        except UnicodeDecodeError:
            continue
    if sample is None:
        encoding = "utf-8-sig"
        sample = raw.decode("utf-8", errors="ignore")

    # conta ; e , para decidir
    semi = sample.count(";")
    comma = sample.count(",")
    sep = ";" if semi >= comma else ","
    return sep, encoding

# ---------- limpeza monet√°ria ----------
def looks_money_series(s: pd.Series) -> bool:
    s2 = s.dropna().astype(str)
    if s2.empty: return False
    return (s2.str.match(MONEY_LIKE, na=False)).mean() >= 0.60

def money_ptbr_to_dot(v: str) -> str:
    if v is None: return v
    t = str(v).strip()
    if t == "" or t == "-": return ""
    if MONEY_LIKE.match(t):
        return t.replace(".", "").replace(",", ".")
    return v

def normalize_money_columns(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty: return df
    for c in df.columns:
        if pd.api.types.is_string_dtype(df[c]) and looks_money_series(df[c]):
            df[c] = df[c].apply(money_ptbr_to_dot)
    return df

# ---------- CSV parsing ----------
def load_csv(path: Path, add_year, dedupe, numeric, verbose):
    sep, enc = sniff_sep_and_encoding(path)
    if verbose:
        print(f"   ‚Üí sep='{sep}' encoding='{enc}'")

    df = pd.read_csv(path, sep=sep, dtype=str, engine="python", encoding=enc)
    df = df.fillna("")

    # remove linhas 'total' (se houver)
    if dedupe:
        before = len(df); df = strip_total_rows(df)
        if verbose and before != len(df): print(f"   linhas 'total' removidas: {before - len(df)}")

    # normaliza nomes: min√∫sculas e troca espa√ßo/h√≠fen/‚Äú ‚Äì ‚Äù por underscore
    def norm_col(c: str) -> str:
        c2 = c.strip().replace("\ufeff","")
        c2 = c2.replace("‚Äì","-").replace("‚Äî","-")
        c2 = c2.lower().replace(" ", "_").replace("-", "_").replace("/", "_")
        c2 = re.sub(r"_+", "_", c2)
        return c2

    df.columns = [norm_col(c) for c in df.columns]

    # padroniza 'exerc√≠cio' ‚Üí 'exercicio'
    if "exerc√≠cio" in df.columns and "exercicio" not in df.columns:
        df = df.rename(columns={"exerc√≠cio":"exercicio"})

    # cria/normaliza 'ano'
    if add_year:
        if "ano" in df.columns:
            df["ano"] = df["ano"].astype(str).str.extract(r"(\d{4})")
        elif "exercicio" in df.columns:
            df["ano"] = df["exercicio"].astype(str).str.extract(r"(\d{4})")
        else:
            y = infer_year(path)
            if y: df["ano"] = str(y)

    # normaliza√ß√£o monet√°ria opcional (continua TEXT na staging)
    if numeric:
        df = normalize_money_columns(df)

    if verbose:
        print(f"   colunas finais: {list(df.columns)}")
        try: print("   preview:\n" + df.head(3).to_string(index=False))
        except: pass
    return df

# ---------- roteamento por pasta ----------
def route_table(root: Path, file_path: Path) -> str:
    rel = file_path.relative_to(root)
    top = rel.parts[0].lower()

    if top in ("receitas",):
        return "stg_receitas"     # anexo10_prev_arrec_YYYY.csv

    if top in ("empenhadas", "despesas_empenhadas"):
        return "stg_despesas_empenhadas"

    if top in ("liquidadas", "despesas_liquidadas"):
        return "stg_despesas_liquidadas"

    if top in ("pagas", "despesas_pagas"):
        return "stg_despesas_pagas"

    return "stg_outros"

# ---------- coer√ß√£o m√≠nima por destino (garante o ‚Äúformato esperado‚Äù) ----------
def coerce_for_table(df: pd.DataFrame, table: str) -> pd.DataFrame:
    # s√≥ reorganiza/garante colunas-chave se elas existem; n√£o cria n√∫meros do nada.
    if table == "stg_receitas":
        # deve ter 8 colunas: ano,codigo,especificacao,subitem,previsao,arrecadacao,para_mais,para_menos
        wanted = ["ano","codigo","especificacao","subitem","previsao","arrecadacao","para_mais","para_menos"]
        have = [c for c in wanted if c in df.columns]
        if len(have) >= 6:  # aceita sem para_mais/para_menos em vers√µes antigas
            df = df[[c for c in wanted if c in df.columns]]
        return df

    if table == "stg_despesas_empenhadas":
        # Exerc√≠cio;Entidade;Empenhado;Estornado;Revers√£o;L√≠quido
        # ap√≥s normaliza√ß√£o ‚Üí exercicio,entidade,empenhado,estornado,revers√£o,l√≠quido  (acentos preservados)
        return df

    if table == "stg_despesas_liquidadas":
        # exercicio, entidade, liquidado_-_or√ßamento, estornado_-_or√ßamento, liquidado_-_restos_a_pagar, estornado_-_restos_a_pagar, l√≠quido
        return df

    if table == "stg_despesas_pagas":
        # exercicio, entidade, pago_-_or√ßamento, estornado_-_or√ßamento, pago_-_restos_a_pagar, estornado_-_restos_a_pagar, l√≠quido
        return df

    return df

# ---------- main ----------
def main():
    args = parse_args()
    root = Path(args.csv).resolve()
    if not root.exists():
        print(f"‚ùå pasta n√£o existe: {root}", file=sys.stderr); sys.exit(1)

    try:
        conn = get_conn()
    except Exception as e:
        print("‚ùå falha na conex√£o Postgres:", e, file=sys.stderr); sys.exit(2)

    files = sorted(root.rglob("*.csv"))
    print(f"‚öôÔ∏è  encontrados {len(files)} CSVs sob {root}")
    total = 0

    for f in files:
        rel = f.relative_to(root)
        target = route_table(root, f)
        print(f"\n‚öôÔ∏è  Carregando {rel} ‚Üí {args.schema}.{target}")
        try:
            df = load_csv(f, args.add_year, args.dedupe, args.numeric, args.verbose)
            df = coerce_for_table(df, target)

            if df.empty:
                print("   (vazio) ‚Äî ignorado."); continue

            ensure_table_and_cols(conn, args.schema, target, list(df.columns), verbose=args.verbose)
            n = insert_df(conn, args.schema, target, df)
            total += n
            print(f"‚úÖ Inserido: {n} linha(s).")
        except Exception as e:
            try: conn.rollback()
            except: pass
            ctx = {"columns": list(df.columns) if 'df' in locals() else None}
            log_error(rel, e, ctx)
            print(f"‚ùå Erro ao processar {rel}: {e}")
            print(f"   ‚Üí veja o log em: logs/{'__'.join(rel.parts)}.log")

    print(f"\nüéØ Conclu√≠do. Linhas inseridas: {total}")

if __name__ == "__main__":
    main()
